# 创建信息传递网络

将卷积操作泛化到不规则域通常表达为_邻域聚合_或_消息传递_方案。设$$\mathbf{x}^{(k-1)}_i \in \mathbb{R}^F$$表示第k-1层中的节点i的特征向量，$$\mathbf{e}_{j，i} \in \mathbb{R}^D$$表示从节点j到节点i的(可选的)边特征向量，消息传递图神经网络可以描述为:

$$
\mathbf{x}_i^{(k)} = \gamma^{(k)} \left( \mathbf{x}_i^{(k-1)}, \bigoplus_{j \in \mathcal{N}(i)} \, \phi^{(k)}\left(\mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)},\mathbf{e}_{j,i}\right) \right),
$$
其中 $$\bigoplus$$ 表示可微分的、排列不变的函数，例如求和、均值或最大值，$$\gamma$$ 和 $$\phi$$ 表示可微分函数如MLP(多层感知机)。

## 消息传递基类"MessagePassing"

PyG提供了:cl:`torch_geometric.nn.conv.message_passing.MessagePassing`基类，它通过自动进行消息传播来帮助创建这样的消息传递图神经网络。用户只需要定义函数$$\phi$$，即:meth:`torch_geometric.nn.conv.message_passing.MessagePassing.message`，和函数$$\gamma$$，即:meth:`torch_geometric.nn.conv.message_passing.MessagePassing.update`，以及要使用的聚合方案，即:o:`aggr="add"`， :o:`aggr="mean"` 或 :o:`aggr="max"`。

这需要借助以下方法：

* :o:`MessagePassing(aggr="add"， flow="source_to_target"， node_dim=-2)`: 定义要使用的聚合方案(:o:`"add"`、:o:`"mean"` 或 :o:`"max"`)和消息传递的流动方向(要么 :o:`"source_to_target"`，要么 :o:`"target_to_source"`)。此外，:o:`node_dim` 属性表示沿着哪个轴进行传播。

* :o:`MessagePassing.propagate(edge_index， size=None， **kwargs)`: 初始化调用以开始传播消息。传入边索引和构造消息及更新节点嵌入所需的所有额外数据。注意:func:`propagate` 不仅限于对形状为 :o:`[N， N]` 的方形邻接矩阵进行消息交换，还可以在一般的稀疏赋值矩阵(例如二分图)上交换消息，形状为 :o:`[N， M]`，通过传递额外的参数 :o:`size=(N， M)`。如果设置为 :o:`None`，则假设赋值矩阵为方形矩阵。对于具有两个独立节点集合及索引的二分图，每个集合携带自己的信息，可以通过以元组形式传递信息来标记此划分，例如 :o:`x=(x_N， x_M)`。

* :o:`MessagePassing.message(...)`: 根据 :o:`flow="source_to_target"` 构造目标节点 i 的消息，对于图中的每条边 (j，i) ∈ E;如果 :o:`flow="target_to_source"`，则对于每条边 (i，j) ∈ E 构造源节点 i 的消息。可以调用 :meth:`propagate` 最初传递的任意参数。另外，传递给 :meth:`propagate` 的张量可以通过在变量名称后附加 :o:`_i` 或 :o:`_j` 来映射到相应的节点 i 和 j，例如 :o:`x_i` 和 :o:`x_j`。我们通常将 i 称为聚合信息的中心节点，将 j 称为相邻节点，因为这是最常见的表示法。

* :o:`MessagePassing.update(aggr_out， ...)`: 对每个节点 i ∈ V，更新节点嵌入，类似于函数$$\gamma$$。第一个参数为聚合输出，后续参数为最初传递给 :fu:`propagate` 的任意参数。

让我们通过重新实现两种流行的 GNN 变体来验证这一点，即 Kipf 和 Welling 的 GCN 层和 Wang 等人的 EdgeConv 层。

## 实现GCN层

GCN层在数学上定义为:

$$
\mathbf{x}_i^{(k)} = \sum_{j \in \mathcal{N}(i) \cup \{ i \}} \frac{1}{\sqrt{\deg(i)} \cdot \sqrt{\deg(j)}} \cdot \left( \mathbf{W}^{\top} \cdot \mathbf{x}_j^{(k-1)} \right) + \mathbf{b},
$$
其中相邻节点的特征先由权重矩阵$$\mathbf{W}$$进行线性变换，再被节点的度数归一化，最后进行求和。最后，我们对聚合的输出应用偏置向量$$\mathbf{b}$$。这个公式可以分为以下步骤:

1. 在邻接矩阵中添加自连接。

2. 线性变换节点特征矩阵。 

3. 计算归一化系数。

4. 在$$\phi$$中归一化节点特征。

5. 求和相邻节点特征(:o:`"add"`聚合)。

6. 应用最终的偏置向量。

步骤1-3通常在消息传递之前计算。步骤4-5可以通过 :cl:`torch_geometric.nn.conv.message_passing.MessagePassing` 基类轻松实现。完整的层实现如下:

```Python
import torch
from torch.nn import Linear， Parameter
from torch_geometric.nn import MessagePassing
from torch_geometric.utils import add_self_loops， degree

class GCNConv(MessagePassing):
    def __init__(self， in_channels， out_channels):
        super().__init__(aggr='add')  # "Add" aggregation (Step 5).
        self.lin = Linear(in_channels， out_channels， bias=False)
        self.bias = Parameter(torch.Tensor(out_channels))

        self.reset_parameters()

    def reset_parameters(self):
        self.lin.reset_parameters()
        self.bias.data.zero_()

    def forward(self， x， edge_index):
        # x has shape [N， in_channels]
        # edge_index has shape [2， E]

        # Step 1: Add self-loops to the adjacency matrix.
        edge_index， _ = add_self_loops(edge_index， num_nodes=x.size(0))

        # Step 2: Linearly transform node feature matrix.
        x = self.lin(x)

        # Step 3: Compute normalization.
        row， col = edge_index
        deg = degree(col， x.size(0)， dtype=x.dtype)
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0
        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]

        # Step 4-5: Start propagating messages.
        out = self.propagate(edge_index， x=x， norm=norm)

        # Step 6: Apply a final bias vector.
        out += self.bias

        return out

    def message(self， x_j， norm):
        # x_j has shape [E， out_channels]

        # Step 4: Normalize node features.
        return norm.view(-1， 1) * x_j
```

:cl:`torch_geometric.nn.conv.GCNConv` 从 :cl:`torch_geometric.nn.conv.message_passing.MessagePassing` 继承，使用 :o:`"add"` 传播。层的所有逻辑在其 :meth:`forward` 方法中。这里，我们首先使用 :meth:`torch_geometric.utils.add_self_loops` 函数为边索引添加自连接(步骤1)，并通过调用 :cl:`torch.nn.Linear` 实例来线性变换节点特征(步骤2)。

归一化系数deriv自每个节点i的节点度$$\text{deg}(i)$$，并为图中每条边(j， i) ∈ E 转换为 $$1/\sqrt{\text{deg}(i)} \cdot \sqrt{\text{deg}(j)}$$(步骤3)。结果保存在形状为 :o:`[num_edges]` 的张量 :o:`norm` 中。

然后我们调用 :meth:`torch_geometric.nn.conv.message_passing.MessagePassing.propagate`，它在内部调用 :meth:`message`、:meth:`aggregate` 和 :m:`update`。我们传递节点嵌入 :o:`x` 和归一化系数 :o:`norm` 作为消息传递的额外参数。

在 :m:`message` 函数中，我们需要用 :o:`norm` 对相邻节点特征 :o:`x_j` 进行归一化。这里，:o:`x_j` 表示一个提升后的张量，其中包含每条边的源节点特征，即每个节点的相邻节点特征。通过在变量名后追加 :o:`_i` 或 :o:`_j` 即可自动提升节点特征。事实上，任何张量都可以通过这种方式转换，只要它们包含源节点或目标节点特征即可。

这就是创建一个简单消息传递层所需的全部内容。你可以将该层用作深度架构的基础模块。初始化和调用该层非常简单:

```python
conv = GCNConv(16， 32)
x = conv(x， edge_index)
```

实现边卷积
----------

边卷积层处理图或点云，其数学定义为:

$$
\mathbf{x}_i^{(k)} = \max_{j \in \mathcal{N}(i)} h_{\mathbf{\Theta}} \left( \mathbf{x}_i^{(k-1)}, \mathbf{x}_j^{(k-1)} - \mathbf{x}_i^{(k-1)} \right)
$$
其中$$h_{\mathbf{\Theta}}$$表示一个MLP。与GCN层类似，我们可以使用 :cl:`torch_geometric.nn.conv.message_passing.MessagePassing` 类来实现该层，这次使用 :o:`"max"` 聚合:

```python
import torch
from torch.nn import Sequential as Seq， Linear， ReLU
from torch_geometric.nn import MessagePassing

cl EdgeConv(MessagePassing):
    def __init__(self， in_channels， out_channels):
        super().__init__(aggr='max') #  "Max" aggregation.
        self.mlp = Seq(Linear(2 * in_channels， out_channels)，
                       ReLU()，
                       Linear(out_channels， out_channels))

    def forward(self， x， edge_index):
        # x has shape [N， in_channels]
        # edge_index has shape [2， E]

        return self.propagate(edge_index， x=x)

    def message(self， x_i， x_j):
        # x_i has shape [E， in_channels]
        # x_j has shape [E， in_channels]

        tmp = torch.cat([x_i， x_j - x_i]， dim=1)  # tmp has shape [E， 2 * in_channels]
        return self.mlp(tmp)
```

在 :m:`message` 函数中，我们对每条边 (j，i) ∈ E 使用 :o:`self.mlp` 来变换目标节点特征 :o:`x_i` 和相对源节点特征 :o:`x_j - x_i`。

边卷积实际上是一个动态卷积，它使用特征空间中的最近邻为每一层重新计算图。幸运的是，PyG提供了一个GPU加速的批量 k-NN 图生成方法 :m:`torch_geometric.nn.pool.knn_graph`:

这个方法可以批量生成基于最近邻的动态图，从而实现特征空间中的动态卷积。

```python
from torch_geometric.nn import knn_graph

cl DynamicEdgeConv(EdgeConv):
    def __init__(self， in_channels， out_channels， k=6):
        super().__init__(in_channels， out_channels)
        self.k = k

    def forward(self， x， batch=None):
        edge_index = knn_graph(x， self.k， batch， loop=False， flow=self.flow)
        return super().forward(x， edge_index)
```

这里，:m:`knn_graph` 计算一个最近邻图，进一步用于调用 :cl:`torch_geometric.nn.conv.EdgeConv` 的 :meth:`forward` 方法。

这给我们提供了一个清晰的接口来初始化和调用该层:

```python
conv = DynamicEdgeConv(3， 128， k=6)
x = conv(x， batch)
```

