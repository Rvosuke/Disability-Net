# A Gentle Introduction to Graph Neural Networks【笔记】

原文见[A Gentle Introduction to Graph Neural Networks (distill.pub)](https://distill.pub/2021/gnn-intro/)

## 图的定义

图由三部分组成:顶点(Vertex)、边(Edge)、全局上下文(Global)。每部分都可以嵌入额外信息,被嵌入的信息可以用向量表示。

对于边还有额外的信息表示:有向边和无向边的区别。

## **用图表示数据**

当用图来表示数据时,我们直觉上适合的数据类型是结构化的数据。但实际上,图也可以表示图像和文本等数据,这些数据的表示方式也很直接,通过图像和文本的图表示来理解图。

**用图表示图像**

图像通常使用张量(三维向量)来表示,即一个矩阵,其中每个元素是一个RGB向量。 

如果要用图来表示图像,我们可以将图像中每个像素表示为图中的一个节点,其RGB值作为节点的特征向量。

另外,可以使用图中的边(Edge)来表示图像中像素之间的连通性,即如果两个像素相邻,则在图中对应节点间应存在一条无向边。

另一种表示方式是使用邻接矩阵,横纵坐标都是每个节点的位置标识。若有N个节点,则构成一个N*N的矩阵,矩阵的值表示两个节点之间的连通性,若连通则值为1,否则为0。

**用图表示文本**

文本可以将单词及符号进行数字索引,这一步骤可看作编码,然后每个编码后的元素作为节点,使用有向边将节点进行连接,得到一个图。

实际上,用图表示图像和文本是多余的,效率不高。我们可以发现,对于图像的图表示中,其邻接矩阵呈现带状结构;文本的图表示中,邻接矩阵只有对角线上有值,这无疑是极大的稀疏矩阵。

**用图表示异构数据** 

对于图表示图像和文本,其结构较为相似。但是有些数据,其节点之间的连通性并不是固定的,则可以方便地使用图进行抽象,换言之,有些数据本质上就是图结构,例如分子结构、社交网络等。

## 图的预测任务

图机器学习的输入和输出不必都是图。这取决于所解决的具体问题和预测的任务类型。让我们详细讨论一下：

1. **节点级别的预测**:
    - **输入**: 通常是一个图，可能带有节点和边的特征。
    - **输出**: 节点的标签或某种属性。例如，在社交网络中，我们可能希望预测每个用户的属性（如年龄、性别或兴趣）。

2. **边级别的预测**:
    - **输入**: 通常是一个图，带有节点和边的特征。
    - **输出**: 边的标签或属性。例如，预测两个用户之间是否会建立社交关系。

3. **图级别的预测**:
    - **输入**: 一个或多个图。每个图可能有其节点和边的特征。
    - **输出**: 整个图的标签或属性。例如，在药物发现中，一个常见的任务是预测化合物的活性，其中每个化合物是一个图。

以下是几种常见的输入和输出组合：
- **节点分类**：输入是图，输出是每个节点的标签。
- **边分类或链接预测**：输入是图，输出是边的标签或可能存在的边的概率。
- **图分类**：输入是多个图，输出是每个图的标签。

所以，不是所有图机器学习任务的输入和输出都是图。但在所有这些情况下，图结构都是核心的，因为它捕获了实体之间的关系，这对于许多预测任务来说都是关键信息。

## 图机器学习的挑战

图机器学习主要有以下挑战：

1. 图表示问题：图结构数据如何用矩阵或张量的形式表示,以便输入神经网络?邻接矩阵会产生稀疏矩阵并存在排列不变性问题。

2. 节点和边的特征表达：如何抽取节点和边的有效特征,并以矢量的形式表示,作为图神经网络的输入?

3. 大小不定的输入：真实世界的图规模参差不齐,如何设计可以处理可变大小图的神经网络架构?

神经网络主要接受向量形式的数据作为输入,而我们之前使用图来抽象现实数据,接下来需要将图抽象为向量表示。

对于节点、边或全局上下文,可以很方便地使用向量来进行表达。但是对于边的连通性,其向量表示就比较困难。直接使用邻接矩阵来表示连通性的方法容易想到,但一般来说边的数量远远少于节点数量的平方,所以得到的邻接矩阵非常稀疏,不利于处理效率和内存使用。因此出现了一种新的表示方式,即构建一个二维矩阵,每个元素表示两个节点之间的连通方向。

这种新的稀疏矩阵表示克服了邻接矩阵的缺点,既表达了图的连通性,又便于输入给神经网络进行处理。接下来的关键是设计可以处理这样图数据表示的神经网络模型,学习图中的特征表示和连接模式。

## 图神经网络

在这里，我们以信息传递神经网络（Message Passing Neural Networks）为例，来详细介绍图神经网络（Graph Neural Networks, GNNs）。

图神经网络需要满足一个核心特性：置换不变性（Permutation Invariance）。如果我们直接将节点、边和全局上下文信息合并，然后使用多层感知机（MLP）进行预测，这样得出的预测结果将依赖于输入数据的顺序，从而不满足置换不变性的要求。为了解决这个问题，GNN设计了独立的共享MLP来分别处理节点、边和全局上下文信息，即一个GNN层由三个独立的MLP模型组成。这种设计确保了输出结果具有置换不变性。

然而，以上方法也存在一个缺点：它忽视了图中节点之间的连通性信息。为了充分挖掘图结构中的信息，我们采用池化（或称为汇聚）方法。

汇聚方法有多种，与卷积神经网络（CNN）中的池化操作类似，包括加和、平均、取极值等。这些汇聚方法在操作上大致相似，例如，在节点信息的汇聚过程中，我们可以将某一节点当前的特征向量与其相邻节点的特征向量进行加和。边的汇聚操作也是类似的。

通过这样的汇聚操作，我们不仅解决了信息传递的问题，还提出了一个新的问题：如何选择合适的汇聚方法。在不同的场景下，我们可以只对节点进行汇聚，也可以只对边进行汇聚，或者同时对节点、边和全局上下文信息进行汇聚。

这样的优化旨在提供更准确和清晰的信息，以便更好地理解图神经网络及其各个组成部分。

