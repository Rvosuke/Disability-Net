# ResNet论文阅读

## 残差学习

**降级问题（Degradation Problem）**：直觉告诉我们，更深的网络应该能够捕获更复杂的特征和信息，因此训练误差应该低于浅层网络。然而，实际情况却并非如此。由于梯度爆炸/消失等问题，随机增加网络深度并不能一致地提高模型性能，有时甚至性能会下降。

为了解决这一降级问题，作者提出了利用**恒等映射（Identity Mapping）**(又称为**底层映射（Underlying Mapping）**)的方法。这种方法允许网络中的一部分输入直接通过到输出，从而缓解因网络深度增加而导致的梯度问题。

更具体地说，考虑网络中的一段结构，这里表示为 $H(x)$。根据**泛函逼近理论（Universal Approximation Theorem）**，我们知道多个非线性层可以近似任何复杂的函数。但如果直接用这些层去逼近$H(x)$，很容易遇到梯度消失或梯度爆炸的问题。因此，作者提出用这些层去逼近残差函数$F(x) = H(x) - x $。这符合恒等映射的思路，即允许部分输入$x $直接传递到输出。于是，原函数 $ H(x) $ 可以重新表示为 $F(x) + x$ ，其中$F(x)$ 是由中间网络层（通常称为残差块）计算出来的。

## 初始化权重

在ResNet（残差网络）的论文中，作者采用了ReLU激活函数以解决深度网络常面临的梯度爆炸或消失问题。这些问题往往与模型参数的初始化方式有关。为了充分挖掘网络结构的潜能并提升性能，作者在每个残差块中嵌入了批量归一化（Batch Normalization, BN）层。

在初始化方面，作者采取了精心设计的策略：
1. **卷积层的权重**：使用Kaiming He初始化，以使权重服从特定的正态分布，进而优化ReLU激活函数在前向和反向传播中的性能。
2. **BN层的参数**：BN不仅仅是一个纯粹的归一化操作，它还包含两个可学习的参数：缩放因子（scale）和位移因子（shift）。这些参数在初始化时被设定为特定的分布，具体地，缩放因子初始化为0.5，而位移因子（偏置）则初始化为0。

通过这种方式，ResNet成功地解决了许多深度学习模型面临的挑战，包括但不限于梯度消失和爆炸问题，从而实现了更高的训练和泛化性能。

